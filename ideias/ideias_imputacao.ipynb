{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Montando um classificador para imputação de dados faltantes ##########################\n",
    "\n",
    "\n",
    "####### Traduzir os dados em dados ordinários, em função da frequência deles (value_counts()) #######\n",
    "\n",
    "state_encode_rev = df_students.State.value_counts().reset_index().reset_index()[[\"index\",\"level_0\"]].to_dict()['index']\n",
    "city_encode_rev = df_students.City.value_counts().reset_index().reset_index()[[\"index\",\"level_0\"]].to_dict()['index']\n",
    "university_encode_rev = df_students.UniversityId.value_counts().reset_index().reset_index()[[\"index\",\"level_0\"]].to_dict()['index']\n",
    "course_encode_rev = df_students.CourseId.value_counts().reset_index().reset_index()[[\"index\",\"level_0\"]].to_dict()['index']\n",
    "\n",
    "state_encode = dict((v,k) for k, v in state_encode_rev.items())\n",
    "state_encode['NaN'] = np.nan\n",
    "city_encode = dict((v,k) for k, v in city_encode_rev.items())\n",
    "city_encode['NaN'] = np.nan\n",
    "university_encode = dict((v,k) for k, v in university_encode_rev.items())\n",
    "university_encode['NaN'] = np.nan\n",
    "course_encode = dict((v,k) for k, v in course_encode_rev.items())\n",
    "course_encode['NaN'] = np.nan\n",
    "\n",
    "\n",
    "ordinal_state_cols_mapping = [{\n",
    "    \"col\":\"State\",    \n",
    "    \"mapping\": state_encode},\n",
    "]\n",
    "ordinal_city_cols_mapping = [{\n",
    "    \"col\":\"City\",    \n",
    "    \"mapping\": city_encode},\n",
    "]\n",
    "ordinal_university_cols_mapping = [{\n",
    "    \"col\":\"UniversityId\",    \n",
    "    \"mapping\": university_encode},\n",
    "]\n",
    "ordinal_course_cols_mapping = [{\n",
    "    \"col\":\"CourseId\",    \n",
    "    \"mapping\": course_encode},\n",
    "]\n",
    "\n",
    "state_encoder = OrdinalEncoder(mapping = ordinal_state_cols_mapping, \n",
    "                         return_df = True)  \n",
    "city_encoder = OrdinalEncoder(mapping = ordinal_city_cols_mapping, \n",
    "                         return_df = True)  \n",
    "university_encoder = OrdinalEncoder(mapping = ordinal_university_cols_mapping, \n",
    "                         return_df = True)  \n",
    "course_encoder = OrdinalEncoder(mapping = ordinal_course_cols_mapping, \n",
    "                         return_df = True) \n",
    "\n",
    "df_students_enc = state_encoder.fit_transform(df_students)\n",
    "df_students_enc = city_encoder.fit_transform(df_students_enc)\n",
    "df_students_enc = university_encoder.fit_transform(df_students_enc)\n",
    "df_students_enc = course_encoder.fit_transform(df_students_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Imputation Using Multivariate Imputation by Chained Equation (MICE)\n",
    "Main steps used in multiple imputations This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. For more information on the algorithm mechanics, you can refer to the Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Para treinar o classificador tipo 1 ########\n",
    "\n",
    "\n",
    "\n",
    "train_enc = df_students_enc[[\"CourseId\",\"UniversityId\",\"State\",\"City\"]]\n",
    "train_enc[train_enc == -1] = np.nan\n",
    "\n",
    "# começa o treino #\n",
    "\n",
    "imputed_training = mice(train_enc.values)\n",
    "rounded_imputed_training = np.round(imputed_training)\n",
    "pd_imputed_data = pd.DataFrame(rounded_imputed_training)\n",
    "\n",
    "####### Terminou o treino sobre os dados e a imputação #######\n",
    "\n",
    "### Renomeia as colunas do novo dataset ###\n",
    "\n",
    "pd_imputed_data = pd_imputed_data.rename(columns = {0:\"CourseId\",1:\"UniversityId\",2:\"State\",3:\"City\"})\n",
    "pd_imputed_data.head(10)\n",
    "\n",
    "####### Decodificando de ordinários para nominais #######\n",
    "\n",
    "pid_state = list(np.array(pd_imputed_data[\"State\"]))\n",
    "pid_city = list(np.array(pd_imputed_data[\"City\"]))\n",
    "pid_uni = list(np.array(pd_imputed_data[\"UniversityId\"]))\n",
    "pid_course = list(np.array(pd_imputed_data[\"CourseId\"]))\n",
    "\n",
    "temp_state = dict((str(key), value) for (key, value) in state_encode_rev.items())\n",
    "temp_city = dict((str(key), value) for (key, value) in city_encode_rev.items())\n",
    "temp_uni = dict((str(key), value) for (key, value) in uni_encode_rev.items())\n",
    "temp_course = dict((str(key), value) for (key, value) in course_encode_rev.items())\n",
    "\n",
    "str_pid_state = [str(x)[0] for x in pid_state]\n",
    "str_pid_city = [str(x)[0] for x in pid_city]\n",
    "str_pid_uni = [str(x)[0] for x in pid_uni]\n",
    "str_pid_course = [str(x)[0] for x in pid_course]\n",
    "\n",
    "\n",
    "for i in str_pid_state:\n",
    "    str_pid_state[str_pid_state.index(i)] = temp_state[i]\n",
    "for i in str_pid_city:\n",
    "    str_pid_city[str_pid_city.index(i)] = temp_city[i]    \n",
    "for i in str_pid_uni:\n",
    "    str_pid_uni[str_pid_uni.index(i)] = temp_uni[i]    \n",
    "for i in str_pid_course:\n",
    "    str_pid_course[str_pid_course.index(i)] = temp_course[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daqui faz o processo reverso, dizer que nao da pra usar sklearn.preprocessing.OrdinalEncoder pq ele nao aceita\n",
    "# NaN's\n",
    "#temp = dict((np.float64(key), value) for (key, value) in state_encode_rev.items())\n",
    "#sort_idx = np.argsort(temp.keys())\n",
    "#idx = np.searchsorted(temp.keys(),pid,sorter = sort_idx)\n",
    "#out = np.asarray(state_encode_rev.values())[sort_idx][idx]\n",
    "#out\n",
    "\n",
    "idx = np.nonzero(temp.keys() == pid[:,None])[1]\n",
    "print(idx)\n",
    "out = np.asarray(temp.values())[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando com get_dummies()\n",
    "df_students_dummies = pd.concat([pd.get_dummies(df_students['CourseId'],prefix='CourseId'),\n",
    "                                 pd.get_dummies(df_students['UniversityId'],prefix='UniversityId'),\n",
    "                                 pd.get_dummies(df_students['State'],prefix='State'),\n",
    "                                 pd.get_dummies(df_students['City'],prefix='City')],\n",
    "                                 axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the MICE training\n",
    "\n",
    "\n",
    "imputed_training=mice(df_students_dummies.astype('float64'))\n",
    "rounded_imputed_training = np.round(imputed_training)\n",
    "pd.DataFrame(rounded_imputed_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_training=mice(train.values)\n",
    "rounded_imputed_training = np.round(imputed_training)\n",
    "pd.DataFrame(rounded_imputed_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_numeric():\n",
    "    pass\n",
    "\n",
    "def translate_to_original():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
